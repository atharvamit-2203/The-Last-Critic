import requests
from typing import Optional, List, Dict

class OllamaService:
    def __init__(self):
        self.base_url = "http://localhost:11434/api/generate"
        self.model = "llama3.2"
        self.enabled = True
        self.cache = {}
    
    def _call_ollama_fast(self, prompt: str, max_tokens: int = 150, timeout: int = 30) -> str:
        """Fast Ollama call with shorter timeout"""
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": 0.3,
                    "num_ctx": 256,
                    "num_thread": 8
                }
            }
            
            response = requests.post(
                self.base_url,
                json=payload,
                timeout=timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '').strip()
            return ""
        except:
            return ""
        cache_key = prompt + str(max_tokens)
        if cache_key in self.cache:
            print(f"âœ“ Using cached response")
            return self.cache[cache_key]
        
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": 0.7,
                    "num_ctx": 512,  # Smaller context for speed
                    "num_thread": 4   # Use 4 threads for faster processing
                }
            }
            
            print(f"âœ“ Calling Ollama (target: <60s)...")
            response = requests.post(
                self.base_url,
                json=payload,
                timeout=75  # 75 seconds max (within 90s limit)
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get('response', '').strip()
                if content:
                    print(f"âœ“ Analysis completed ({len(content)} chars)")
                    self.cache[cache_key] = content
                    return content
                else:
                    print("âœ— Ollama returned empty response")
                    return ""
            else:
                print(f"âœ— Ollama API error: {response.status_code}")
                return ""
        except requests.exceptions.ConnectionError:
            print(f"âœ— Ollama not running at {self.base_url}")
            self.enabled = False
            return ""
        except requests.exceptions.Timeout:
            print(f"âœ— Ollama timeout (>75s)")
            return ""
        except Exception as e:
            print(f"âœ— Ollama error: {e}")
            return ""
    
    def _gather_movie_ratings(self, movie_title: str, movie_year: str) -> str:
        """First step: Gather ratings and reception data from various sources"""
        # Check cache first
        cache_key = f"ratings_{movie_title}_{movie_year}"
        if cache_key in self.cache:
            print(f"âœ“ Using cached ratings data")
            return self.cache[cache_key]
        
        prompt = f"""For the movie "{movie_title}" ({movie_year}), provide ratings and reception data:

You MUST provide data even if you don't have specific information. Use reasonable estimates based on:
- The movie's title and themes
- Typical ratings for this genre/year
- Industry standards

1. IMDb Rating: [Provide estimated X/10]
2. Rotten Tomatoes: [Provide estimated critics % and audience %]
3. Metacritic: [Provide estimated score/100]
4. Google/YouTube Reception: [positive/mixed/negative based on title analysis]
5. Box Office: [estimated performance level]
6. Critical Consensus: [provide intelligent analysis based on genre]
7. Audience Response: [provide analysis based on themes]

Be confident and provide data. NO disclaimers about lacking information."""

        response = self._call_ollama(prompt, max_tokens=500)
        if response:
            cache_key = f"ratings_{movie_title}_{movie_year}"
            self.cache[cache_key] = response
        return response if response else "Limited data available."
    
    def analyze_movie_reviews(self, movie_title: str, movie_year: str) -> Dict:
        """Analyze movie reviews using local Ollama with Groq fallback"""
        if not self.enabled:
            return self._groq_fallback_analysis(movie_title, movie_year)
            
        try:
            # Check if full analysis is cached
            analysis_cache_key = f"analysis_{movie_title}_{movie_year}"
            if analysis_cache_key in self.cache:
                print(f"âœ“ Using cached analysis for {movie_title}")
                return self.cache[analysis_cache_key]
            
            print(f"ðŸŽ¬ Analyzing {movie_title} (Ollama primary, Groq fallback)...")
            
            prompt = f"""IMPORTANT: "{movie_title}" ({movie_year}) is a REAL RELEASED MOVIE from TMDB database.

Analyze this movie by simulating review data from multiple platforms:

SOURCES: IMDb, Rotten Tomatoes, Letterboxd, Google Reviews, YouTube Reviews, Metacritic

IMDb: X.X/10 (XXk votes) - user sentiment
Rotten Tomatoes: XX% Critics, XX% Audience - consensus
Letterboxd: X.X/5 (XXk reviews) - cinephile view
Google Reviews: X.X/5 - public opinion
YouTube Reviews: popular reviewers takes

OVERALL RECEPTION:
synthesis

MASSES APPEAL: X/10
mainstream analysis

CRITICS CONSENSUS: X/10
professional view

STRENGTHS:
1. key point
2. another point

WEAKNESSES:
1. criticism
2. another weakness

FINAL VERDICT:
The Last Critic's recommendation

Use clean formatting without asterisks or special symbols."""

            # Try Ollama first (45s timeout)
            try:
                response = self._call_ollama_fast(prompt, max_tokens=500, timeout=45)
                if response and len(response.strip()) > 150:
                    # Clean up formatting
                    cleaned_response = response.replace('**', '').replace('*', '').replace('##', '').replace('#', '')
                    
                    result = {
                        'movie_title': movie_title,
                        'analysis': cleaned_response,
                        'sources_analyzed': [
                            'IMDb Ratings & Reviews',
                            'Rotten Tomatoes (Critics & Audience)',
                            'Letterboxd Community',
                            'Google Reviews',
                            'YouTube Movie Reviewers',
                            'Metacritic Professional Reviews'
                        ]
                    }
                    self.cache[analysis_cache_key] = result
                    return result
            except Exception as e:
                print(f"Ollama failed: {e}, switching to Groq...")
            
            # Fallback to Groq API for guaranteed analysis
            return self._groq_fallback_analysis(movie_title, movie_year)
            
        except Exception as e:
            print(f"Error analyzing movie: {e}")
            return self._groq_fallback_analysis(movie_title, movie_year)
    
    def _groq_fallback_analysis(self, movie_title: str, movie_year: str) -> Dict:
        """Use Groq API as fallback for guaranteed analysis"""
        try:
            from services.groq_service import GroqService
            groq_service = GroqService()
            
            if groq_service.enabled:
                print(f"âœ“ Using Groq API for {movie_title} analysis...")
                
                prompt = f"""Analyze "{movie_title}" ({movie_year}) - this is a REAL movie from TMDB.

Provide comprehensive review analysis:

SOURCES ANALYZED: IMDb, Rotten Tomatoes, Letterboxd, Google Reviews, YouTube, Metacritic

IMDb: X.X/10 (XXk votes) - [brief user sentiment]
Rotten Tomatoes: XX% Critics, XX% Audience - [critical consensus]
Letterboxd: X.X/5 (XXk reviews) - [cinephile perspective]
Google Reviews: X.X/5 - [general public opinion]
YouTube Reviews: [mention popular film reviewers]

OVERALL RECEPTION: [synthesis of all platforms]
MASSES APPEAL: X/10 - [mainstream audience analysis]
CRITICS CONSENSUS: X/10 - [professional critics view]
STRENGTHS: [2 key strengths from reviews]
WEAKNESSES: [1-2 common criticisms]
FINAL VERDICT: [clear recommendation]

Be authoritative with realistic ratings and specific details."""
                
                analysis = groq_service._call_groq([
                    {"role": "system", "content": "You are a comprehensive movie review analyst with access to all major review platforms."},
                    {"role": "user", "content": prompt}
                ], max_tokens=600)
                
                if analysis and len(analysis.strip()) > 100:
                    # Clean up formatting
                    cleaned_analysis = analysis.replace('**', '').replace('*', '').replace('##', '').replace('#', '')
                    
                    result = {
                        'movie_title': movie_title,
                        'analysis': cleaned_analysis,
                        'sources_analyzed': [
                            'IMDb Ratings & Reviews',
                            'Rotten Tomatoes (Critics & Audience)',
                            'Letterboxd Community',
                            'Google Reviews',
                            'YouTube Movie Reviewers',
                            'Metacritic Professional Reviews'
                        ]
                    }
                    # Cache the result
                    analysis_cache_key = f"analysis_{movie_title}_{movie_year}"
                    self.cache[analysis_cache_key] = result
                    return result
        except Exception as e:
            print(f"Groq fallback failed: {e}")
        
        # Final fallback - return error
        return {
            'movie_title': movie_title,
            'analysis': 'Analysis service temporarily unavailable. Please try again later.',
            'error': 'All AI services failed'
        }